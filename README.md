# ğŸ“˜ Mini-CLIP: Contrastive Imageâ€“Text Retrieval from Scratch (PyTorch)

A minimal, fully open-source implementation of a **CLIP-style Visionâ€“Language Model** trained from scratch on the **MS-COCO imageâ€“caption dataset**.  
This project demonstrates how contrastive multimodal learning can align images and text into a shared embedding space, enabling **cross-modal retrieval**.

---

## ğŸš€ Highlights

- **ViT-B/16** visual encoder + **Transformer** text encoder  
- **Symmetric InfoNCE** contrastive loss with **hard negative mining**  
- **Mixed-precision training** (optional)  
- **Full COCO support** (train/val splits)  
- **FAISS-powered retrieval evaluation** for instant Recall@K  
- **Efficient batching + padded text sequences**  
- Tiny toy dataset for instant debugging  
- 100% PyTorch â€” no external CLIP dependencies

---

# ğŸ§  Model Overview

### ğŸ–¼ Image Encoder
- ViT-B/16 backbone (ImageNet-pretrained) with frozen classification head  
- Linear projection into the shared embedding space  
- L2-normalized embeddings for cosine similarity  

### âœï¸ Text Encoder
- Whitespace tokenizer + learnable vocabulary  
- Learned token + positional embeddings  
- Multi-layer Transformer encoder (GELU + dropout)  
- `[BOS]` state projected into the shared embedding space and normalized  

### ğŸ”— Contrastive Loss (InfoNCE)
For a batch size *N*:

- Compute similarity matrix **S = image_emb @ text_embáµ€**
- Apply cross-entropy loss in both directions (imageâ†’text & textâ†’image)  
- Add a **margin-based hard negative term** that selects the top-*k* most confusing mismatched pairs every batch  
- Total loss = symmetric InfoNCE + weighted hard-negative penalty

### âš¡ Retrieval with FAISS
- Encoded image/text features are indexed with `faiss.IndexFlatIP`
- Instant **Imageâ†’Text** and **Textâ†’Image** Recall@K using cosine similarity search  
- Requires `faiss-cpu` (installed via `pip install -r requirement.txt`)

---

# ğŸ“¦ Dataset

This project uses:

### **MS-COCO 2014**
- ~82k training images (`train2014/`)  
- ~40k validation images (`val2014/`)  
- Each image has 5 human-written captions  

COCO is converted into a simple JSON format:

```json
[
  {
    "image": "COCO_train2014_000000000009.jpg",
    "caption": "A man riding a bike on a city street."
  }
]
```

---

# ğŸ‹ï¸ Training & Evaluation

Train on COCO subset:

```bash
python -m src.train --config configs/default.yaml
```

Evaluate on COCO val:

```bash
python -m src.eval \
  --config configs/eval_coco.yaml \
  --checkpoint checkpoints/clip_epoch10.pt
```

---

# ğŸ›  Project Structure

```
Mini-CLIP-from-Scratch/
|-- src/
|   |-- model.py         # Image/Text encoders + CLIP head
|   |-- dataset.py       # COCO + toy dataset loaders
|   |-- train.py         # Training loop entrypoint
|   |-- eval.py          # Retrieval evaluation script
|   `-- utils.py         # Helper utilities
|-- configs/
|   `-- default.yaml     # Training configuration
|-- data/
|   |-- coco/            # Raw COCO images (gitignored)
|   |-- images/          # Toy/demo images
|   `-- train_captions.json
|-- checkpoints/         # Saved CLIP weights
|-- scripts/
|   |-- make_toy_dataset.py
|   `-- run_train.sh
|-- requirement.txt      # Python dependencies
`-- README.md
```
